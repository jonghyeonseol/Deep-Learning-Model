# Modern Deep Learning Framework ğŸš€

**State-of-the-Art Deep Learning Implementation for CIFAR-10 Image Classification**

This project implements modern deep learning architectures and training techniques used in 2024-2025 research and production systems.

---

## ğŸ¯ What's New

### âœ¨ **3 Modern Architectures**
- **ResNet**: Residual networks with skip connections (ResNet-18, 34, 50)
- **EfficientNet**: Mobile-optimized with depthwise separable convolutions + SE attention
- **CNN-Transformer**: Hybrid combining CNNs and self-attention mechanisms

### âš¡ **Modern Training Techniques**
- **AdamW Optimizer**: Decoupled weight decay (better than Adam)
- **Cosine Annealing with Warmup**: Smooth LR scheduling
- **Mixed Precision (AMP)**: 2-3x faster training on GPU
- **Exponential Moving Average (EMA)**: +0.5% accuracy improvement
- **Gradient Clipping**: Training stability
- **Label Smoothing**: Better calibration

### ğŸ“Š **Advanced Data Augmentation**
- **RandAugment**: Automated augmentation policies
- **MixUp**: Linear interpolation between samples
- **CutMix**: Cut-and-paste augmentation
- **Cutout / Random Erasing**: Occlusion robustness

### ğŸ›¡ï¸ **Modern Regularization**
- **DropBlock**: Structured dropout for CNNs
- **Stochastic Depth**: Random layer dropping
- **Label Smoothing**: Confidence calibration

---

## ğŸš€ Quick Start

### 1. Activate Environment
```bash
source venv/bin/activate
```

### 2. Train with Modern Techniques

#### **Option A: ResNet-18 (Recommended for beginners)**
```bash
python main_modern.py --model resnet18 --modern-training --epochs 50
```
Expected: **93-94% accuracy** in ~10 minutes (GPU)

#### **Option B: EfficientNet (Best efficiency)**
```bash
python main_modern.py --model efficientnet_b0 --modern-training --augmentation randaugment --epochs 100
```
Expected: **94-95% accuracy** with fewer parameters

#### **Option C: CNN-Transformer (Cutting-edge)**
```bash
python main_modern.py --model cnn_transformer_base --activation gelu --modern-training --epochs 100
```
Expected: **93-94% accuracy** with self-attention

---

## ğŸ“‹ Available Models

| Model | Parameters | Speed | Accuracy (50 epochs) | Use Case |
|-------|-----------|-------|---------------------|----------|
| `cnn` | 1.2M | âš¡âš¡âš¡ | 86-88% | Baseline |
| `resnet_tiny` | 2M | âš¡âš¡âš¡ | 88-90% | Quick experiments |
| `resnet18` | 11M | âš¡âš¡ | 93-94% | **Best overall** |
| `resnet34` | 21M | âš¡ | 93-94% | Deeper network |
| `resnet50` | 23M | âš¡ | 94-95% | Highest accuracy |
| `efficientnet_tiny` | 1M | âš¡âš¡âš¡ | 89-91% | Mobile/edge |
| `efficientnet_b0` | 4M | âš¡âš¡ | 94-95% | **Best efficiency** |
| `efficientnet_b1` | 6M | âš¡âš¡ | 94-95% | Scaled up |
| `cnn_transformer_small` | 8M | âš¡ | 92-93% | Attention learning |
| `cnn_transformer_base` | 16M | âš¡ | 93-94% | Full hybrid |
| `vit_tiny` | 5M | âš¡ | 91-92% | Pure transformer |

---

## ğŸ“ Learning Path

### **Level 1: Understand Basics**
```bash
# Original basic CNN
python main.py --activation relu --epochs 10

# See available activations
python main.py --list-activations
```

### **Level 2: Modern Architecture**
```bash
# Train ResNet (adds residual connections + batch norm)
python main_modern.py --model resnet18 --epochs 20

# Compare with original CNN
python main_modern.py --model cnn --epochs 20
```
**Learning**: Residual connections, batch normalization, global average pooling

### **Level 3: Modern Training**
```bash
# Without modern training
python main_modern.py --model resnet18 --epochs 30

# With modern training
python main_modern.py --model resnet18 --modern-training --epochs 30
```
**Learning**: AdamW, cosine annealing, mixed precision, EMA, label smoothing

### **Level 4: Advanced Augmentation**
```bash
# Standard augmentation
python main_modern.py --model resnet18 --augmentation standard --modern-training

# RandAugment
python main_modern.py --model resnet18 --augmentation randaugment --modern-training

# With Cutout
python main_modern.py --model resnet18 --augmentation randaugment --cutout --modern-training
```
**Learning**: RandAugment, Cutout, data augmentation impact

### **Level 5: Mobile-Optimized Networks**
```bash
# EfficientNet with depthwise separable convolutions
python main_modern.py --model efficientnet_b0 --modern-training
```
**Learning**: Depthwise separable convolutions, SE attention, parameter efficiency

### **Level 6: Attention Mechanisms**
```bash
# CNN-Transformer hybrid
python main_modern.py --model cnn_transformer_base --modern-training --epochs 100
```
**Learning**: Self-attention, transformers, patch embeddings, positional encoding

### **Level 7: Comprehensive Comparison**
```bash
# Compare all models
python main_modern.py --compare-models --epochs 50

# Full benchmark
python benchmark_all.py --full
```
**Learning**: Architecture comparison, ablation studies, performance analysis

---

## ğŸ“Š Benchmark & Comparison

### Compare Different Approaches

```bash
# Compare architectures
python benchmark_all.py --architectures --epochs 20

# Compare training techniques (modern vs standard)
python benchmark_all.py --techniques --epochs 20

# Compare activation functions
python benchmark_all.py --activations --epochs 20

# Quick test (2 epochs per experiment)
python benchmark_all.py --quick --architectures
```

### Expected Performance Improvements

| Technique | Baseline | With Technique | Improvement |
|-----------|----------|----------------|-------------|
| Standard â†’ Modern Training | 91% | 93% | **+2%** |
| Adam â†’ AdamW | 91% | 92% | **+1%** |
| Step LR â†’ Cosine Annealing | 91% | 92% | **+1%** |
| No AMP â†’ AMP | Same | Same | **2-3x faster** |
| No EMA â†’ EMA | 92% | 92.5% | **+0.5%** |
| No Label Smoothing â†’ Label Smoothing (0.1) | 92% | 93% | **+1%** |
| Basic Aug â†’ RandAugment | 91% | 93% | **+2%** |
| No MixUp â†’ MixUp | 92% | 93.5% | **+1.5%** |
| **All Combined** | **91%** | **94-95%** | **+3-4%** |

---

## ğŸ”§ Detailed Usage

### List All Models
```bash
python main_modern.py --list-models
```

### Train Specific Model
```bash
python main_modern.py \
  --model resnet18 \
  --activation swish \
  --modern-training \
  --epochs 50 \
  --batch-size 128 \
  --lr 0.001
```

### With Advanced Augmentation
```bash
python main_modern.py \
  --model efficientnet_b0 \
  --augmentation randaugment \
  --cutout \
  --mixup \
  --modern-training
```

### Quick Test (2 epochs)
```bash
python main_modern.py --model resnet_tiny --quick
```

---

## ğŸ“ Project Structure

```
Deep-Learning-Model/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ network.py              # Original CNN
â”‚   â”œâ”€â”€ resnet.py               # âœ¨ ResNet architectures
â”‚   â”œâ”€â”€ efficientnet.py         # âœ¨ EfficientNet architectures
â”‚   â”œâ”€â”€ cnn_transformer.py      # âœ¨ CNN-Transformer hybrid
â”‚   â””â”€â”€ activations.py          # Custom activation functions
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ trainer.py              # Standard trainer
â”‚   â”œâ”€â”€ modern_trainer.py       # âœ¨ Modern trainer (AdamW, AMP, EMA)
â”‚   â”œâ”€â”€ augmentation.py         # âœ¨ Advanced augmentation
â”‚   â”œâ”€â”€ regularization.py       # âœ¨ DropBlock, Stochastic Depth
â”‚   â”œâ”€â”€ data_loader.py          # CIFAR-10 data loader
â”‚   â””â”€â”€ visualization.py        # Training plots
â”œâ”€â”€ main.py                     # Original training script
â”œâ”€â”€ main_modern.py              # âœ¨ Modern training script
â”œâ”€â”€ benchmark_all.py            # âœ¨ Comprehensive benchmark
â”œâ”€â”€ MODERN_DL_GUIDE.md          # âœ¨ Detailed guide
â”œâ”€â”€ README_MODERN.md            # âœ¨ This file
â””â”€â”€ CLAUDE.md                   # Project instructions
```

---

## ğŸ¯ Expected Results (CIFAR-10, 50 epochs)

### With Modern Training

| Model | Test Accuracy | Training Time (GPU) | Parameters |
|-------|--------------|-------------------|-----------|
| ResNet-18 | 93-94% | ~10 min | 11M |
| ResNet-50 | 94-95% | ~20 min | 23M |
| EfficientNet-B0 | 94-95% | ~15 min | 4M |
| CNN-Transformer | 93-94% | ~25 min | 16M |

### Without Modern Training (Standard)

| Model | Test Accuracy | Training Time (GPU) |
|-------|--------------|-------------------|
| ResNet-18 | 91-92% | ~15 min |
| Basic CNN | 85-87% | ~5 min |

---

## ğŸ’¡ Key Concepts Explained

### 1. **Residual Connections** (ResNet)
```python
# Instead of learning F(x), learn residual F(x) - x
output = F(x) + x  # Skip connection
```
**Why**: Easier to optimize, enables very deep networks (100+ layers)

### 2. **Depthwise Separable Convolutions** (EfficientNet)
```python
# Standard conv: HÃ—WÃ—C_inÃ—C_out operations
# Depthwise separable: HÃ—WÃ—C_in + C_inÃ—C_out operations
# ~9x fewer operations!
```
**Why**: Mobile-friendly, parameter efficient

### 3. **Squeeze-and-Excitation** (Attention)
```python
# Learn channel-wise attention weights
weights = Sigmoid(FC(GlobalAvgPool(x)))
output = x * weights  # Reweight channels
```
**Why**: Focus on important channels, ignore noise

### 4. **Self-Attention** (Transformer)
```python
# Each patch attends to all other patches
attention_weights = Softmax(Q @ K^T / sqrt(d))
output = attention_weights @ V
```
**Why**: Model long-range dependencies, global context

### 5. **AdamW** (Optimizer)
```python
# Adam: weight_decay affects gradient normalization âŒ
# AdamW: weight_decay is decoupled âœ…
w = w - lr * gradient - lr * weight_decay * w
```
**Why**: Proper regularization, better generalization

### 6. **Mixed Precision** (AMP)
```python
# Use float16 for speed, float32 for stability
with autocast():
    output = model(input)  # Automatic dtype selection
```
**Why**: 2-3x faster, less memory, no accuracy loss

---

## ğŸ“ What You'll Learn

By experimenting with this project, you'll understand:

### **Modern Architectures**
- âœ… Residual connections and why they work
- âœ… Batch normalization for stable training
- âœ… Depthwise separable convolutions for efficiency
- âœ… Attention mechanisms (SE and self-attention)
- âœ… Patch embeddings and positional encoding

### **Training Techniques**
- âœ… AdamW vs Adam optimizer differences
- âœ… Learning rate scheduling (warmup + cosine decay)
- âœ… Mixed precision training (float16 + float32)
- âœ… Exponential moving average for better models
- âœ… Gradient clipping for stability

### **Regularization**
- âœ… Label smoothing for calibration
- âœ… DropBlock vs standard dropout
- âœ… Stochastic depth for deep networks
- âœ… Weight decay best practices

### **Data Augmentation**
- âœ… RandAugment automated policies
- âœ… MixUp and CutMix sample mixing
- âœ… Cutout and random erasing
- âœ… When to use each technique

### **PyTorch Best Practices**
- âœ… Efficient data loading
- âœ… GPU memory optimization
- âœ… Training loop best practices
- âœ… Model checkpointing and loading

---

## ğŸ“š Further Reading

### Essential Papers (in reading order)

1. **Batch Normalization** (2015) - Understanding modern training
2. **ResNet** (2015) - Residual connections
3. **MobileNet** (2017) - Depthwise separable convolutions
4. **Squeeze-and-Excitation** (2018) - Channel attention
5. **EfficientNet** (2019) - Compound scaling
6. **Vision Transformer** (2020) - Transformers for images
7. **AdamW** (2019) - Proper weight decay
8. **MixUp** (2017) - Sample mixing
9. **CutMix** (2019) - Spatial mixing
10. **RandAugment** (2019) - Automated augmentation

See [MODERN_DL_GUIDE.md](MODERN_DL_GUIDE.md) for full paper references.

---

## â“ FAQ

**Q: Which model should I start with?**
A: Start with `resnet18` - good balance of performance and speed.

**Q: Do I need a GPU?**
A: CPU works but is slow. GPU recommended for modern training (AMP requires CUDA).

**Q: What's the minimum GPU memory needed?**
A: 4GB for ResNet-18, 6GB for EfficientNet-B0, 8GB for CNN-Transformer

**Q: How long does training take?**
A: ResNet-18 (50 epochs): ~10 min on GPU, ~2 hours on CPU

**Q: What accuracy should I expect?**
A: With modern training: 93-95% on CIFAR-10 (50-100 epochs)

**Q: Can I use this for other datasets?**
A: Yes! Modify `num_classes` and data loader. Works for any image classification task.

---

## ğŸ¤ Contributing

This is an educational project. Feel free to:
- Add new architectures (Vision Transformer variants, ConvNeXt, etc.)
- Implement additional techniques (SAM optimizer, LAMB, etc.)
- Improve documentation
- Add more datasets

---

## ğŸ“„ License

MIT License - Free for educational and research use

---

## ğŸ™ Acknowledgments

This project implements techniques from modern deep learning research:
- ResNet architecture from Microsoft Research
- EfficientNet from Google Brain
- Transformer architecture from Google Research
- Various optimization and regularization techniques from the community

---

**Happy Learning! ğŸš€**

For detailed explanations of each technique, see [MODERN_DL_GUIDE.md](MODERN_DL_GUIDE.md)
